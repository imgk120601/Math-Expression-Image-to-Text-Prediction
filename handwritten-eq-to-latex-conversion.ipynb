{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7049508,"sourceType":"datasetVersion","datasetId":4056853},{"sourceId":7248406,"sourceType":"datasetVersion","datasetId":4199337},{"sourceId":8070071,"sourceType":"datasetVersion","datasetId":4761600}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":77.753798,"end_time":"2023-11-26T05:02:47.209667","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-26T05:01:29.455869","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models\nfrom tqdm import tqdm\nimport numpy as np\nimport PIL\nimport random\nfrom nltk.tokenize import word_tokenize\nfrom PIL import Image\nimport pandas as pd\n\n\nimport math\nfrom nltk import word_tokenize\nfrom collections import Counter\nfrom nltk.util import ngrams\n\n\nclass BLEU(object):\n    @staticmethod\n    def compute(candidate, references, weights):\n        candidate = [c.lower() for c in candidate]\n        references = [[r.lower() for r in reference] for reference in references]\n\n        p_ns = (BLEU.modified_precision(candidate, references, i) for i, _ in enumerate(weights, start=1))\n        s = math.fsum(w * math.log(p_n) for w, p_n in zip(weights, p_ns) if p_n)\n\n        bp = BLEU.brevity_penalty(candidate, references)\n        return bp * math.exp(s)\n\n    @staticmethod\n    def modified_precision(candidate, references, n):\n        counts = Counter(ngrams(candidate, n))\n\n        if not counts:\n            return 0\n\n        max_counts = {}\n        for reference in references:\n            reference_counts = Counter(ngrams(reference, n))\n            for ngram in counts:\n                max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n\n        clipped_counts = dict((ngram, min(count, max_counts[ngram])) for ngram, count in counts.items())\n\n        return sum(clipped_counts.values()) / sum(counts.values())\n    \n    @staticmethod\n    def brevity_penalty(candidate, references):\n        c = len(candidate)\n        # r = min(abs(len(r) - c) for r in references)\n        r = min(len(r) for r in references)\n\n        if c > r:\n            return 1\n        else:\n            return math.exp(1 - r / c)\n\ndef give_score(grount_truths, predictions):\n    scorer = BLEU()\n\n\n    overall = 0\n    for gt, pred in zip(grount_truths, predictions):\n        gt = gt.split()\n        pred = pred.split()\n        overall += BLEU.compute(pred,[gt], weights=[1/4, 1/4, 1/4, 1/4])\n\n    print(\"Macro Bleu : \", overall/len(predictions))\n\n# Device configuration\n#device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cuda')\ndevice\n\n","metadata":{"papermill":{"duration":6.760413,"end_time":"2023-11-26T05:01:39.951168","exception":false,"start_time":"2023-11-26T05:01:33.190755","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:36.562871Z","iopub.execute_input":"2024-04-09T06:27:36.563413Z","iopub.status.idle":"2024-04-09T06:27:46.977679Z","shell.execute_reply.started":"2024-04-09T06:27:36.563380Z","shell.execute_reply":"2024-04-09T06:27:46.976795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image_path):\n    # Open the image file\n    img = Image.open(image_path)\n    \n    img = img.resize((224, 224))\n\n    img_array = np.array(img)\n    new_im = np.stack([img_array, img_array, img_array], axis = 2)\n\n    \n    # Convert the image to a PyTorch tensor and normalize\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Converts the image to a PyTorch tensor\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    img_tensor = transform(new_im)\n    return img_tensor","metadata":{"papermill":{"duration":0.014449,"end_time":"2023-11-26T05:01:39.971780","exception":false,"start_time":"2023-11-26T05:01:39.957331","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:46.979720Z","iopub.execute_input":"2024-04-09T06:27:46.980554Z","iopub.status.idle":"2024-04-09T06:27:46.986145Z","shell.execute_reply.started":"2024-04-09T06:27:46.980520Z","shell.execute_reply":"2024-04-09T06:27:46.985312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mydata(Dataset):\n    def __init__(self, csv_path, path):\n        self.path = path\n        self.csv_path = csv_path\n        self.vocab = []\n        self.form = []\n        self.img = []\n        self.dataframe = None\n        self.preprocess()\n\n    def preprocess(self):\n        df = pd.read_csv(self.csv_path)\n        self.dataframe = df\n        training_dataset = df['formula']\n        all_tokens = []\n        for formula in training_dataset:    \n            for token in formula.split():\n                all_tokens.append(token)\n\n        vocab = [\"<pad>\", \"<sos>\", \"<eos>\"]\n        vocab.extend(list(set(all_tokens)))\n        token_to_index = {token: idx for idx, token in enumerate(vocab)}\n        self.vocab = token_to_index\n\n        indexed_dataset = []\n        for formula in training_dataset:\n            pres = [token_to_index[\"<sos>\"]]\n            for j in formula.split():\n                pres.append(token_to_index[j])\n            pres.append(token_to_index[\"<eos>\"])\n            indexed_dataset.append(pres)\n\n        max_sequence_length = max(len(formula) for formula in indexed_dataset)\n        padded_sequences = [formula + [0] * (max_sequence_length - len(formula)) for formula in indexed_dataset]\n        padded_sequences_tensor = torch.tensor(padded_sequences)\n        self.form = padded_sequences\n    \n    def __getitem__(self, index):\n        curr = self.dataframe['image'][index]\n        res_im = preprocess_image(self.path + \"images/train/\" + curr)\n        return {\"formula\":self.form[index], \"image\": res_im}\n\n    def __len__(self):\n        return len(self.dataframe)\n\ndef collate_fn(batch):\n    # Extract individual elements from the batch\n    formulas = [item[\"formula\"] for item in batch]\n    images = [item[\"image\"] for item in batch]\n    formulas_tensor = torch.tensor(formulas, dtype=torch.long)\n    \n    # Assuming your images are already in tensor format\n    images_tensor = torch.stack(images)\n\n    return {\"formula\": formulas_tensor, \"image\": images_tensor}\n","metadata":{"papermill":{"duration":0.021422,"end_time":"2023-11-26T05:01:39.998945","exception":false,"start_time":"2023-11-26T05:01:39.977523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:46.987817Z","iopub.execute_input":"2024-04-09T06:27:46.988089Z","iopub.status.idle":"2024-04-09T06:27:47.001745Z","shell.execute_reply.started":"2024-04-09T06:27:46.988067Z","shell.execute_reply":"2024-04-09T06:27:47.000961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Encoder (CNN) architecture\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(32, 64, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(64, 128, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(128, 256, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(256, 512, kernel_size=5),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.AvgPool2d(kernel_size=3, stride=1)\n        )\n\n    def forward(self, x):\n        temp = self.cnn(x)\n        temp = temp.squeeze()\n        return temp\n","metadata":{"papermill":{"duration":0.016179,"end_time":"2023-11-26T05:01:40.020853","exception":false,"start_time":"2023-11-26T05:01:40.004674","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:47.004329Z","iopub.execute_input":"2024-04-09T06:27:47.004916Z","iopub.status.idle":"2024-04-09T06:27:47.020315Z","shell.execute_reply.started":"2024-04-09T06:27:47.004884Z","shell.execute_reply":"2024-04-09T06:27:47.019562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Decoder (LSTM) architecture\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, input_size):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(input_size + embedding_dim, hidden_dim, batch_first=True, bidirectional= True)\n        self.output_layer = nn.Linear(2*hidden_dim, vocab_size)\n        self.softmax = nn.LogSoftmax(dim = 1)\n\n    def forward(self, context, prev_hidden, prev_cell, prev_word):\n        embedded = self.embedding(prev_word)\n        ''' shape of embedded: 64 x 1 x 512 '''\n        #embedded = embedded.squeeze(dim=1)\n        ''' shape of lstm_input: 64 x 1 x 1024 '''\n        lstm_input = torch.cat((context, embedded), dim=-1)\n        output, (hidden , cell) = self.lstm(lstm_input, (prev_hidden, prev_cell))\n        '''\n            output shape: 64 x 1 x 512\n            hidden shape: 1 x 64 x 512\n            cell shape: 1 x 64 x 512\n        '''\n#         print(\"output shape, \", output.shape)\n        output = self.output_layer(output)\n        #prediction = self.softmax(output)\n        return (hidden, cell), output","metadata":{"papermill":{"duration":0.01623,"end_time":"2023-11-26T05:01:40.042784","exception":false,"start_time":"2023-11-26T05:01:40.026554","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:47.021273Z","iopub.execute_input":"2024-04-09T06:27:47.021504Z","iopub.status.idle":"2024-04-09T06:27:47.034777Z","shell.execute_reply.started":"2024-04-09T06:27:47.021484Z","shell.execute_reply":"2024-04-09T06:27:47.033992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass MyModel(nn.Module):\n    def __init__(self, encoder, decoder, vocab_size):\n        super(MyModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.vocab_size = vocab_size\n\n    def forward(self, source, target, teacher_forcing= 0.5):\n        ''' src shape = 64 x 3 x 224 x 224 '''\n        ''' trg shape = 64 x 88 '''\n        batch_size = source.shape[0]\n        target_len = target.shape[1]\n\n        context= self.encoder(source)\n        context = context.unsqueeze(dim=1)\n        \n        \n        hidden = torch.zeros(2, batch_size, 512).to(device)\n        cell = torch.zeros(2, batch_size, 512).to(device)\n#         hidden = context.clone()\n#         cell = context.clone()\n#         hidden = hidden.permute(1, 0, 2)\n#         cell = cell.permute(1, 0, 2)\n#         hidden = hidden.repeat(2, 1, 1)\n#         cell = cell.repeat(2, 1, 1)\n#         print(\"Actual hidden shape \", hidden.shape)\n        outputs = torch.zeros(target_len, batch_size, self.vocab_size).to(device)\n\n        input = target[:, 0]\n        for t in range(1, target_len):\n            ''' shape of input: 64 x 1 '''\n            input = input.unsqueeze(1)\n#             print(\"shape of input: \", input.shape)\n            (hidden, cell), output = self.decoder(context, hidden, cell, input)\n            output = output.squeeze(dim=1)\n            outputs[t] = output\n            teacher_mc = random.random() < teacher_forcing\n            top1 = output.argmax(dim=1)\n            input = target[:, t] if teacher_mc else top1\n        return outputs","metadata":{"papermill":{"duration":0.018448,"end_time":"2023-11-26T05:01:40.067256","exception":false,"start_time":"2023-11-26T05:01:40.048808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:47.035773Z","iopub.execute_input":"2024-04-09T06:27:47.036025Z","iopub.status.idle":"2024-04-09T06:27:47.050308Z","shell.execute_reply.started":"2024-04-09T06:27:47.036004Z","shell.execute_reply":"2024-04-09T06:27:47.049461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, dataload, optimizer, criteria):\n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(tqdm(dataload)):\n        src = batch[\"image\"].to(device)\n        trg = batch[\"formula\"].to(device)\n        \n        \n        '''For parallel data'''\n#         src = batch[\"image\"].cuda()\n#         trg = batch[\"formula\"].cuda()\n        # print(\"src shape: \", src.shape)\n        # print(\"trg shape: \", trg.shape)\n        optimizer.zero_grad()\n        output = model(src, trg)\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n\n        trg = trg.T[1:]\n        trg = trg.reshape((trg.shape[0]*trg.shape[1], ))\n\n        loss = criteria(output, trg)\n        loss.backward()\n\n        optimizer.step()\n        epoch_loss += loss.item()\n    print(\"loss: \", epoch_loss/len(dataload))\n    return epoch_loss/len(dataload)\n","metadata":{"papermill":{"duration":0.015585,"end_time":"2023-11-26T05:01:40.088483","exception":false,"start_time":"2023-11-26T05:01:40.072898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:47.051406Z","iopub.execute_input":"2024-04-09T06:27:47.051711Z","iopub.status.idle":"2024-04-09T06:27:47.064015Z","shell.execute_reply.started":"2024-04-09T06:27:47.051682Z","shell.execute_reply":"2024-04-09T06:27:47.063285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/handwritten-eq-to-latex-conv-dataset/HandwrittenData/\"\n\ntrain_data = Mydata(\"/kaggle/input/handwritten-eq-to-latex-conv-dataset/HandwrittenData/train_hw.csv\", path)\ntrain_dataloader = DataLoader(train_data, batch_size=64, shuffle=False, collate_fn = collate_fn)\n","metadata":{"papermill":{"duration":0.412144,"end_time":"2023-11-26T05:01:40.506379","exception":false,"start_time":"2023-11-26T05:01:40.094235","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:47.065159Z","iopub.execute_input":"2024-04-09T06:27:47.065450Z","iopub.status.idle":"2024-04-09T06:27:47.461782Z","shell.execute_reply.started":"2024-04-09T06:27:47.065430Z","shell.execute_reply":"2024-04-09T06:27:47.460998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder= Encoder()\ndecoder = Decoder(vocab_size=len(train_data.vocab), embedding_dim=512, hidden_dim=512, input_size = 512)\nModel = MyModel(encoder=encoder, decoder=decoder, vocab_size=len(train_data.vocab)).to(device)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=Model.parameters(), lr=0.001)\n\n","metadata":{"papermill":{"duration":6.476601,"end_time":"2023-11-26T05:01:46.989598","exception":false,"start_time":"2023-11-26T05:01:40.512997","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:47.462871Z","iopub.execute_input":"2024-04-09T06:27:47.463154Z","iopub.status.idle":"2024-04-09T06:27:48.004995Z","shell.execute_reply.started":"2024-04-09T06:27:47.463129Z","shell.execute_reply":"2024-04-09T06:27:48.004204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 11\nloss = []\nfor i in range(epochs):\n    curr = train(Model, train_dataloader, optimizer, loss_fn)\n    loss.append(curr)","metadata":{"papermill":{"duration":0.012713,"end_time":"2023-11-26T05:01:47.008344","exception":false,"start_time":"2023-11-26T05:01:46.995631","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:27:48.007932Z","iopub.execute_input":"2024-04-09T06:27:48.008246Z","iopub.status.idle":"2024-04-09T06:53:29.015338Z","shell.execute_reply.started":"2024-04-09T06:27:48.008220Z","shell.execute_reply":"2024-04-09T06:53:29.014320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = {\n    'model_state_dict': Model.state_dict(),\n    'vocab': train_data.vocab,\n    # Add any other information you want to save\n}\n\n# Specify the path where you want to save the model\ntorch.save(checkpoint, 'Model1.pth')","metadata":{"papermill":{"duration":0.10546,"end_time":"2023-11-26T05:01:47.119770","exception":false,"start_time":"2023-11-26T05:01:47.014310","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:29.016531Z","iopub.execute_input":"2024-04-09T06:53:29.016804Z","iopub.status.idle":"2024-04-09T06:53:29.104039Z","shell.execute_reply.started":"2024-04-09T06:53:29.016781Z","shell.execute_reply":"2024-04-09T06:53:29.103128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006131,"end_time":"2023-11-26T05:01:47.132347","exception":false,"start_time":"2023-11-26T05:01:47.126216","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(Model, vocab, image_path):\n    source = preprocess_image(image_path).unsqueeze(dim = 0).to(device)    \n    context = Model.encoder(source).to(device)\n    context = context.unsqueeze(dim = 0)\n    context = context.unsqueeze(dim = 0)\n    hidden = torch.zeros(2, 1, 512).to(device)\n    cell = torch.zeros(2, 1, 512).to(device)\n    \n    \n    input = torch.tensor([ 1]).to(device)\n    end = torch.tensor([2]).to(device)\n    \n    formula = torch.tensor([ 1 ]).to(device)\n    len = 0\n    while torch.equal(input, end) == False and formula.shape[0] < 120:\n        input = input.unsqueeze(1)\n        (hidden, cell), output = Model.decoder(context, hidden, cell, input)\n        output = output.squeeze(dim=1)\n        top1 = output.argmax(dim=1)\n        #res = top1.squeeze()\n        formula = torch.cat((formula, top1), dim = 0)\n        input = top1\n        len += 1\n    torch.cat((formula, end), dim = 0)\n    \n    formula.to('cpu')\n    res = \"\"\n    for i in range(formula.shape[0]):\n        res += get_token(vocab, formula[i])\n        res += \" \"\n\n    return res\n\n","metadata":{"papermill":{"duration":0.018647,"end_time":"2023-11-26T05:01:47.157195","exception":false,"start_time":"2023-11-26T05:01:47.138548","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:29.105178Z","iopub.execute_input":"2024-04-09T06:53:29.105463Z","iopub.status.idle":"2024-04-09T06:53:29.116071Z","shell.execute_reply.started":"2024-04-09T06:53:29.105440Z","shell.execute_reply":"2024-04-09T06:53:29.115059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**loading","metadata":{}},{"cell_type":"code","source":"checkpoint = torch.load('/kaggle/input/mtp-model-1/Model1.pth')\ndecoder = Decoder(vocab_size=len(train_data.vocab), embedding_dim=512, hidden_dim=512, input_size = 512)\nModel = MyModel(encoder=Encoder(), decoder=decoder, vocab_size=len(train_data.vocab)).to(device)\nModel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"papermill":{"duration":1.446621,"end_time":"2023-11-26T05:01:48.609838","exception":false,"start_time":"2023-11-26T05:01:47.163217","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:29.117279Z","iopub.execute_input":"2024-04-09T06:53:29.117529Z","iopub.status.idle":"2024-04-09T06:53:29.781718Z","shell.execute_reply.started":"2024-04-09T06:53:29.117507Z","shell.execute_reply":"2024-04-09T06:53:29.780830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_token(dictionary, search_value):\n    for key, value in dictionary.items():\n        if value == search_value:\n            return key\n    # If the value is not found, you might want to handle this case accordingly.\n    raise ValueError(f\"Value '{search_value}' not found in the dictionary\")","metadata":{"papermill":{"duration":0.025167,"end_time":"2023-11-26T05:01:48.641341","exception":false,"start_time":"2023-11-26T05:01:48.616174","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:29.783068Z","iopub.execute_input":"2024-04-09T06:53:29.783395Z","iopub.status.idle":"2024-04-09T06:53:29.788041Z","shell.execute_reply.started":"2024-04-09T06:53:29.783369Z","shell.execute_reply":"2024-04-09T06:53:29.787204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_data = Mydata(\"/kaggle/input/handwritten-eq-to-latex-conv-dataset/HandwrittenData/val_hw.csv\", \"/kaggle/input/handwritten-eq-to-latex-conv-dataset/HandwrittenData/\")\nvalidation_dataloader = DataLoader(validation_data, batch_size=64, shuffle=False, collate_fn = collate_fn)","metadata":{"papermill":{"duration":0.060921,"end_time":"2023-11-26T05:01:48.711002","exception":false,"start_time":"2023-11-26T05:01:48.650081","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:29.789115Z","iopub.execute_input":"2024-04-09T06:53:29.789737Z","iopub.status.idle":"2024-04-09T06:53:29.841191Z","shell.execute_reply.started":"2024-04-09T06:53:29.789712Z","shell.execute_reply":"2024-04-09T06:53:29.840479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_path = \"/kaggle/input/handwritten-eq-to-latex-conv-dataset/HandwrittenData/images/train/\"\ntarget = []\nreq = []\ndef validation(csv_path):\n    #new addition\n#     target = [] \n#     req = []\n    df = pd.read_csv(csv_path)\n    print(\"total len \", len(df))\n#     for i in range(0, len(df)):\n    for i in range(len(df)):\n        if(i>10):#--new addtion\n            break\n        print(i)\n        if(i == 114):\n            continue\n        image = df[\"image\"][i]\n        target.append(str(df[\"formula\"][i]))\n        req.append(predict(Model, train_data.vocab, images_path + image))\n           \n    print(give_score(target, req))\n        ","metadata":{"papermill":{"duration":0.018941,"end_time":"2023-11-26T05:01:48.738856","exception":false,"start_time":"2023-11-26T05:01:48.719915","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:29.842088Z","iopub.execute_input":"2024-04-09T06:53:29.842347Z","iopub.status.idle":"2024-04-09T06:53:29.848864Z","shell.execute_reply.started":"2024-04-09T06:53:29.842325Z","shell.execute_reply":"2024-04-09T06:53:29.847940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation(\"/kaggle/input/handwritten-eq-to-latex-conv-dataset/HandwrittenData/val_hw.csv\")","metadata":{"papermill":{"duration":56.001951,"end_time":"2023-11-26T05:02:44.747532","exception":false,"start_time":"2023-11-26T05:01:48.745581","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:29.849835Z","iopub.execute_input":"2024-04-09T06:53:29.850062Z","iopub.status.idle":"2024-04-09T06:53:30.821298Z","shell.execute_reply.started":"2024-04-09T06:53:29.850042Z","shell.execute_reply":"2024-04-09T06:53:30.820444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(target[0])\nprint(req[0])\nprint(req[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:53:30.822474Z","iopub.execute_input":"2024-04-09T06:53:30.822826Z","iopub.status.idle":"2024-04-09T06:53:30.827987Z","shell.execute_reply.started":"2024-04-09T06:53:30.822790Z","shell.execute_reply":"2024-04-09T06:53:30.827123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path1=\"/kaggle/input/handwritten-eq-to-latex-conv-dataset/HandwrittenData/images/train/MfrDB3500.png\"\nresult1=predict(Model,train_data.vocab,image_path1)\nprint(result1)","metadata":{"papermill":{"duration":0.025733,"end_time":"2023-11-26T05:02:44.799208","exception":false,"start_time":"2023-11-26T05:02:44.773475","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-09T06:53:30.829041Z","iopub.execute_input":"2024-04-09T06:53:30.829331Z","iopub.status.idle":"2024-04-09T06:53:30.913425Z","shell.execute_reply.started":"2024-04-09T06:53:30.829309Z","shell.execute_reply":"2024-04-09T06:53:30.912501Z"},"trusted":true},"execution_count":null,"outputs":[]}]}